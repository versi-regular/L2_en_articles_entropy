# L2_en_articles_entropy

This repository contains the codebase for an ongoing research into article entropy effects in L2 learners of English conducted at the Faculty of Philosophy of the University of Novi Sad (Serbia).

Considering this an interim Read Me keeping track of a messy, non-optimized code. :)

Scripts are numbered by the order I ran them in:
1. `01_tmx_to_txt_per_language.py`: This script expects a .tmx file of the [SETimes corpus](http://opus.nlpl.eu/SETIMES.php) containing [sentence aligments for Serbian and English](http://opus.nlpl.eu/download.php?f=SETIMES/v2/xml/en-sr.xml.gz) under the name `en-sr.tmx`. As it is ran, the script will ask whether you want to process the English `en` or Serbian `sr` data. The output of this script are the language data per language without the meta-tags, with a sentence per line.
2. `02_tokenize_and_tag.py`: This script expects a .txt file, `tokenizer_input_en.txt` or `tokenizer_input_sr.txt` generated by the previous script. It tokenizes and PoS tags the texts. NOTE: Tokenization and tagging for Serbian ran for over 72 hrs for me (could be my code optimization or the reliance on other libraries, I never did get around to check). I am happy to share the output of this directly if you need it.
3. `03_calculate_ngrams.py`: This script  expects a .txt file, `tokenizer_input_en.txt` (for now) generated by the first script. It uses NLTK to build bi-grams of the language data.
