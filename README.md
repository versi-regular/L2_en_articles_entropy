# L2_en_articles_entropy

This repository contains the codebase for an ongoing research into article entropy effects in L2 learners of English conducted at the Faculty of Philosophy of the University of Novi Sad (Serbia).

Considering this an interim Read Me keeping track of a messy, non-optimized code. :)

Scripts are numbered by the order I ran them in:
1. `01_tmx_to_txt_per_language.py`: This script expects a .tmx file of the [SETimes corpus](http://opus.nlpl.eu/SETIMES.php) containing [sentence aligments for Serbian and English](http://opus.nlpl.eu/download.php?f=SETIMES/v2/xml/en-sr.xml.gz) under the name `en-sr.tmx`. As it is ran, the script will ask whether you want to process the English `en` or Serbian `sr` data. The output of this script are the language data per language without the meta-tags, with a sentence per line.
2. `02_tokenize_and_tag.py`: This script expects a .txt file, `tokenizer_input_en.txt` or `tokenizer_input_sr.txt` generated by the previous script. It tokenizes and PoS tags the texts. It needs NLTK, and the [ReLDI tokeniser](https://github.com/clarinsi/reldi-tokeniser) and [ReLDI tagger](https://github.com/clarinsi/reldi-tagger). NOTE: Tokenization and tagging for Serbian ran for over 72 hrs for me (could be my code optimization or the reliance on other libraries, I never did get around to check). I am happy to share the output of this directly if you need it.
3. `03_pos_tagged_ngrams.py`: This script  expects a .txt file, `tokenizer_output_en.txt` (works only for English for now) generated by the previous script. It filters for bigrams with nouns as their second element and extracts bigrams with their respectice PoS tags.
